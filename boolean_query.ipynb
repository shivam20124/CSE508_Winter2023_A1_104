{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "!pip install --user -U nltk\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import matplotlib\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import nltk.data\n",
    "import numba\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is establishing the preprocessing part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocessing(text):\n",
    "    # text=text[0:100]\n",
    "    #Lowercasing the text\n",
    "    lower=text.lower()\n",
    "\n",
    "    #Splitting into tokens\n",
    "    tokens=lower.split()\n",
    "\n",
    "    #Removing stop words\n",
    "    without_stop = []\n",
    "    for tok in tokens:\n",
    "        if tok not in en_stopwords:\n",
    "            without_stop.append(tok)\n",
    "\n",
    "    #Removing Punctuations and Blank Space tokens\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    final_words=tokenizer.tokenize(' '.join(without_stop))\n",
    "    \n",
    "    return final_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the code for Boolean Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import copy \n",
    "\n",
    "uni_invert_index = open('D:/College/SEMESTER 6/IR/CSE508_Winter2023_A1_104/Unigram_Inverted_Index_File', 'rb')     \n",
    "word_dict = pickle.load(uni_invert_index)\n",
    "uni_invert_index.close()\n",
    "\n",
    "def create_querry(words, operations):\n",
    "  ans = \"\"\n",
    "  for i in range(len(words)-1):\n",
    "    ans += words[i]+\" \"+operations[i]+\" \"\n",
    "  ans = ans+words[-1]\n",
    "  return ans\n",
    "\n",
    "def name_and(word3,file_list):\n",
    "    filenames = []\n",
    "    filelist =[]\n",
    "    list_for_doc1 = word_dict[word3][1]\n",
    "    num_comparison = min(len(list_for_doc1), len(file_list))\n",
    "    for key in list_for_doc1:\n",
    "        if key in file_list:\n",
    "            filelist.append(key)\n",
    "            name = \"cranfield\"+ \"0\"*(4-len(str(key))) + str(key)\n",
    "            filenames.append( name )\n",
    "    return filenames, filelist, num_comparison\n",
    "\n",
    "def name_or(word3,file_list):\n",
    "    filelist = copy.deepcopy(file_list)\n",
    "    list_for_doc1 = word_dict[word3][1]\n",
    "    filenames = []\n",
    "    num_comparison = len(filelist) + len(list_for_doc1)\n",
    "    for key in list_for_doc1:\n",
    "        if(key not in filelist):\n",
    "          filelist.append(key)     \n",
    "    for key in filelist:\n",
    "      name = \"cranfield\"+ \"0\"*(4-len(str(key))) + str(key)\n",
    "      filenames.append(name)\n",
    "    return filenames, filelist, num_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_and_not(word3, file_list):\n",
    "    all_files_list = []\n",
    "    for s in range(1400):\n",
    "        all_files_list.append(s+1)\n",
    "    _file_list = copy.deepcopy(file_list)\n",
    "    filelist = []\n",
    "    filenames = []\n",
    "    list_for_doc1 = word_dict[word3][1]\n",
    "    \n",
    "    num_comp = min(len(_file_list), min(list_for_doc1), 1400-len(_file_list), 1400-len(list_for_doc1))\n",
    "    \n",
    "    for key in _file_list:\n",
    "        if key in list_for_doc1:\n",
    "            _file_list.remove(key)\n",
    "            \n",
    "    for key in _file_list:\n",
    "        name = \"cranfield\"+ \"0\"*(4-len(str(key))) + str(key)\n",
    "        filenames.append(name)\n",
    "    return filenames, _file_list, num_comp\n",
    "\n",
    "def name_or_not(word3, file_list):\n",
    "    all_files_list = []\n",
    "    for s in range(1400):\n",
    "        all_files_list.append(s+1)\n",
    "    filelist = copy.deepcopy(file_list)\n",
    "    list_for_doc1 = word_dict[word3][1]\n",
    "    not_list_for_doc1 = all_files_list\n",
    "    filenames = []\n",
    "    for key in list_for_doc1:\n",
    "            not_list_for_doc1.remove(key)\n",
    "    num_comparison = len(filelist) + len(not_list_for_doc1)\n",
    "    for key in not_list_for_doc1:\n",
    "        if key not in filelist:\n",
    "            filelist.append(key)\n",
    "    for key in filelist:\n",
    "        name = \"cranfield\"+ \"0\"*(4-len(str(key))) + str(key)\n",
    "        filenames.append(name)\n",
    "    return filenames, filelist, num_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_search(arr, x):\n",
    "#     comparisons = 0\n",
    "#     left = 0\n",
    "#     right = len(arr) - 1\n",
    "#     while left <= right:\n",
    "#         mid = (left + right) // 2\n",
    "#         comparisons += 1\n",
    "#         if arr[mid] == x:\n",
    "#             return comparisons\n",
    "#         elif arr[mid] < x:\n",
    "#             left = mid + 1\n",
    "#         else:\n",
    "#             right = mid - 1\n",
    "#     return -1\n",
    "\n",
    "\n",
    "# def total_comparisons(docs, query):\n",
    "#     total_comps = 0\n",
    "#     for word in query.split():\n",
    "#         if word in docs:\n",
    "#             total_comps += binary_search(docs[word], query)\n",
    "#     return total_comps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_input():\n",
    "    seq = input(\"Enter input seq: \")\n",
    "    operations = [x for x in input(\"Enter commma seperated operation: \").split(',')]\n",
    "    lis = preprocessing(seq)\n",
    "    # querry = create_querry(lis,operations)\n",
    "    return lis, operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forming the structure of the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "Number of queries is: 1\n",
      "1. Input Sequence is: ['drops', 'fifth', 'inspection', 'similitude']\n",
      "1. List of operators is: ['AND', 'OR', 'AND NOT']\n"
     ]
    }
   ],
   "source": [
    "print(\"INPUT:\")\n",
    "num_queries = int(input(\"Enter the number of queries you want to perform: \"))\n",
    "print(\"Number of queries is: \" + str(num_queries))\n",
    "list_of_input_words = []\n",
    "list_of_input_operators = []\n",
    "for j in range(num_queries):\n",
    "    input_words, input_operators = take_input()\n",
    "    print(str(j+1) + \". Input Sequence is:\", end= \" \")\n",
    "    print(input_words)\n",
    "    print(str(j+1) + \". List of operators is:\", end=\" \")\n",
    "    print(input_operators)\n",
    "    list_of_input_words.append(input_words)\n",
    "    list_of_input_operators.append(input_operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: drops AND fifth OR inspection AND NOT similitude\n",
      "Number of documents retrieved for query 1: 2\n",
      "Name of the documents retrieved for query 1: cranfield1033, cranfield0730,  \n",
      "Number of comparisons required for query 1: 10\n"
     ]
    }
   ],
   "source": [
    "for k in range(num_queries):\n",
    "    words_k = list_of_input_words[k]\n",
    "    operators_k = list_of_input_operators[k]\n",
    "    query_k = create_querry(words_k, operators_k)\n",
    "    print(\"Query \" + str(k+1) + \":\", end = \" \")\n",
    "    print(query_k)\n",
    "    num_op = len(operators_k)\n",
    "    list1 = word_dict[words_k[0]][1]\n",
    "    name_docs =[]\n",
    "    number_of_comparisons = 0\n",
    "    \n",
    "    for op in range(num_op):\n",
    "\n",
    "        if(operators_k[op] == \"AND\"):\n",
    "          cur_name_docs,list1, num_com = name_and(words_k[op+1], list1)\n",
    "          name_docs = cur_name_docs\n",
    "          number_of_comparisons = number_of_comparisons + num_com\n",
    "          \n",
    "        elif(operators_k[op] == \"OR\"):\n",
    "          cur_name_docs, list1, num_com = name_or(words_k[op+1], list1)\n",
    "          name_docs = cur_name_docs\n",
    "          number_of_comparisons = number_of_comparisons + num_com\n",
    "          \n",
    "        elif(operators_k[op] == \"AND NOT\"):\n",
    "          cur_name_docs, list1, num_com = name_and_not(words_k[op+1], list1)\n",
    "          name_docs = cur_name_docs\n",
    "          number_of_comparisons = number_of_comparisons + num_com\n",
    "               \n",
    "        elif(operators_k[op] == \"OR NOT\"):\n",
    "          cur_name_docs, list1, num_com = name_or_not(words_k[op+1], list1)\n",
    "          name_docs = cur_name_docs\n",
    "          number_of_comparisons = number_of_comparisons + num_com\n",
    "      \n",
    "    name_docs_set = [*set(name_docs)]      \n",
    "    print(\"Number of documents retrieved for query \" + str(k+1) + \":\", end=\" \")\n",
    "    print(len(name_docs_set))\n",
    "    print(\"Name of the documents retrieved for query \" + str(k+1) + \":\", end = \" \")\n",
    "    for items in name_docs_set:\n",
    "      print(items + \",\", end = \" \")\n",
    "    print(\" \")\n",
    "    print(\"Number of comparisons required for query \" + str(k+1) + \": \" + str(number_of_comparisons))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a97dfee85535fb1d5b47fe9558cac8ab9061198b8187c5e3fe351ac320e8c61d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
